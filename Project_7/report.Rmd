---
title: "Project 7"
output: html_notebook
---

## Experiment Design

There are two options a user can choose for Udacity courses. The first is a "free trial", the second is "access course materials". When they click on the first option, they will be asked to enter their payment data and enrolled in a 14 days trial period. After that, they will be charged until they cancel their subscription.
For the second option, they are able to view all course material for free, but they will not receive coaching and cannot submit their final projects (and get a course certificate).

Udacity tested how a change of the free trial period would change the users behavior. After clicking on the "free trial"-button, the users were asked how much time they would be able to spent on the course. If that was 5 hours or more, they were shown they were forwarded to the page where they have to enter their payment data. If they entered less than 5 hours, a warning was displayed that the course usually requires a commitment of more than 5 hours and accessing the free course material might be the better option. Still, they had the option to enroll in the free trial instead.

The hypothesis was that this would reduce the number of frustrated students who left the free trial because they realized they didn't have enough time, but the number of students who would continue past the free trial would not be reduced.

As a unit of diversion cookies were used. After students enrolled, they can be identified by their user-id. It's not possible to enroll in the same fre trial more than once.

### Metric Choice

I will first list the metrics with an explanation for each. Then I will present my choices for invariant and evaluation metrics.

* Number of cookies: That is, number of unique cookies to view the course overview page
* Number of user-ids: That is, number of users who enroll in the free trial.
* Number of clicks: That is, number of unique cookies to click the "Start free trial" button (which happens before the free trial screener is trigger)
* Click-through-probability:  That is, number of unique cookies to click the "Start free trial" button divided by number of unique cookies to view the course overview page
* Gross conversion: That is, number of user-ids to complete checkout and enroll in the free trial divided by number of unique cookies to click the "Start free trial" button.
* Retention: That is, number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by number of user-ids to complete checkout.
* Net conversion: That is, number of user-ids to remain enrolled past the 14-day
boundary (and thus make at least one payment) divided by the number of unique
cookies to click the "Start free trial" button.

#### Invariant metrics
Invariant metrics shouldn't change across the experimental and the control group. They can be used to check the integrity of the measurement after the experiment.

The main goal of this experiment is to see how users react to the new query message displayed after they want to join for the trial. Therefore, the following variables do not change:

* Number of cookies: The number represents the unique visitors of the website. The aim is to split them between the experimental and the control group.
* Number of clicks: The experiment is focusing on the part after the "Start free trial" button has been pressed. Therefore, the number of clicks should be the same across the experimental and the control group.
* Click-through-probability: This probability is the number of clicks divided by the number of cookies. As both of these variables are invariant, the click-through-probability is invariant as well.

#### Evaluation metrics
Evaluation metrics are expected to show the changes across the experimental and the control group. For this experiment we have the following evaluation metrics:

* Gross conversion: The hypothesis is that the gross conversion for the experimental group is lower than the gross conversion for the control group.
* Net conversion: The net conversion is the evaluation metric representing the number of students who keep subscribed after the free trial period. The hypothesis is that it doesn't change significantly for the experimental group.

#### Other
* Number of user-ids
* Retention

The number of user-ids alone does not give us any valuable information. We are interested in the number of people who actually enroll.

### Measuring Standard Deviation
* Unique cookies to view page per day:	40000
* Unique cookies to click "Start free trial" per day:	3200
* Enrollments per day:	660
* Click-through-probability on "Start free trial":	0.08
* Probability of enrolling, given click:	0.20625
* Probability of payment, given enroll:	0.53
* Probability of payment, given click:	0.1093125

In the experiment, we predict we need 5000 cookies oer day in each group. With the given numbers above and a scale factor of ``r 5000/40000``, this results in ``r 3200*0.125`` users who click the "Start free trial"-button and ``r 660*0.125`` enrollments.

##### Gross conversion
```{r eval=FALSE}
gconv_sd <- sqrt(0.2063 * (1-0.2063)/400)
gross_sd <- round(gconv_sd,4)
```

The SD for gross conversion is `r gross_sd`.

##### Net conversion
```{r eval=FALSE}
clicks_sd <- sqrt(0.1093 * (1-0.1093)/400)
clicks_sd <- round(clicks_sd,4)
```

The SD for net conversion is `r clicks_sd`.

### Sizing
#### Number of Samples vs. Power
##### Gross conversion
* Baseline conversion rate:	20.625%
* Minimum effect: 1%
* $\alpha$: 0.05
* $\beta$: 80%
* Sample size: 25.835
* click-through-probability: 0.08


```{r}
25835/0.08*2
```

##### Net conversion
* Baseline conversion rate:	10.93%
* Minimum effect: 0.75%
* Sample size: 27.411

```{r}
27411/0.08*2
```

As the project should have appropriate numbersfor both metrics, we have to use the higher number of pageviews. So we have to get 685.275 pageviews.

#### Duration vs. Exposure
According to the numbers given above, there are 40000 unique visitors per day. To have 6825275 pageviews, the experiment will have to run for ``r round(6825275/40000,0)`` days.
The free-trial period is 14 days, so the total amount of time this experiment will take is 32 days. I think this is a reasonable timespan.

I do not think that the experiment is risky in any way. The question is not ethically problematic and is not sensitive. It might be possible to draw some conclusions from the information. For example, if users are able to spend 30 hours on the course, it's unlikely that they have a full-time job. 

There can be a small financial risk for Udacity. If we assume that less visitors subscribe for the free trial when they are asked how much time they can spent, there might also be less visitors who continue the course after the free trial.

## Experiment Analysis
### Sanity Checks
```{r eval=FALSE, echo=FALSE}
control <- read.table("Control.csv", sep=",", header = TRUE)
experiment <- read.table("Experiment.csv", sep=",", header = TRUE)
```

#### Number of cookies
```{r eval=FALSE, echo=FALSE}
cViews <- sum(control$Pageviews)
eViews <- sum(experiment$Pageviews)
sd_cookies <- round(sqrt((0.5*0.5)/(cViews+eViews)),4)
me_cookies <- sd_cookies*1.96
lowerBound_cookies <- 0.5 - me_cookies
upperBound_cookies <- 0.5 + me_cookies
observed_cookies <- round(cViews/(cViews+eViews),4)
```

* Lower bound: `r round(lowerBound_cookies ,4)`
* Upper bound: `r round(upperBound_cookies ,4)`
* Confidence interval: [`r round(0.5- me_cookies,4)`, `r round(0.5 + me_cookies,4)`]
* Observed probability: `r observed_cookies`

#### Number of clicks
```{r eval=FALSE, echo=FALSE}
cClicks <- sum(control$Clicks)
eClicks <- sum(experiment$Clicks)
sd_clicks <- round(sqrt((0.5*0.5)/(cClicks+eClicks)),4)
me_clicks <- sd_clicks*1.96
lowerBound_clicks <- 0.5 - me_clicks
upperBound_clicks <- 0.5 + me_clicks
observed_clicks <- cClicks/(cClicks+eClicks)
```

* Lower bound: `r round(lowerBound_clicks,4)`
* Upper bound: `r round(upperBound_clicks,4)`
* Confidence interval: [`r round(0.5- me_clicks,4)`, `r round(0.5 + me_clicks,4)`]
* Observed probability: `r round(observed_clicks,4)`

#### Click-through-probability
```{r eval=FALSE, echo=FALSE}
click_throughCTRL <- cClicks/cViews
sd_clthrough <- sqrt(click_throughCTRL*(1-click_throughCTRL)/cViews)
me_clickthrough <- sd_clthrough*1.96
lowerBound_clickthrough <- 0.5 - me_clickthrough
upperBound_clickthrough <- 0.5 + me_clickthrough
observed_clickthrough <- eClicks/eViews
```

* Lower bound: `r round(lowerBound_clickthrough,4)`
* Upper bound: `r round(upperBound_clickthrough,4)`
* Confidence interval: [`r round(click_throughCTRL - me_clickthrough,4)`, `r round(click_throughCTRL + me_clickthrough,4)`]
* Observed probability: `r round(observed_clickthrough,4)`

All observed values fall in the expected range, so all metrics pass the sanity check.

### Result Analysis
#### Effect Size Tests

```{r eval=FALSE, echo=FALSE}
control_na <- control[complete.cases(control),]
experiment_na <- experiment[complete.cases(experiment),]

NcEnrollment <- sum(control_na$Enrollments, na.rm=T)
NeEnrollment <- sum(experiment_na$Enrollments, na.rm=T)
NcPayments <- sum(control_na$Payments, na.rm=T)
NePayments <- sum(experiment_na$Payments, na.rm=T)
NcClicks <- sum(control_na$Clicks, na.rm=T)
NeClicks <- sum(experiment_na$Clicks, na.rm=T)
```

##### Gross conversion
```{r eval=FALSE, echo=FALSE}
gc <- (NcEnrollment + NeEnrollment)/(NcClicks + NeClicks)
gc_se <- sqrt(gc*(1-gc)*(1/NcClicks+1/NeClicks))
gc_me <- gc_se *1.96
gc_diff <- (NeEnrollment/NeClicks) - (NcEnrollment/NcClicks)
```

*  Difference: `r round(gc_diff,4)`
* 95% confidence interval: [`r round(gc_diff - gc_me,4)`, `r round(gc_diff + gc_me,4)`]

The difference is statistically significant as the confidence interval does not include Zero. It is also practically significant as it does not include dmin.

##### Net conversion
```{r eval=FALSE, echo=FALSE}
nc <- (NcPayments + NePayments)/(NcClicks + NeClicks)
nc_se <- sqrt(nc*(1-nc)*(1/NcClicks+1/NeClicks))
nc_me <- nc_se *1.96
nc_diff <- (NePayments/NeClicks) - (NcPayments/NcClicks)
```

*  Difference: `r round(nc_diff,4)`
* 95% confidence interval: [`r round(nc_diff - nc_me,4)`, `r round(nc_diff + nc_me,4)`]

The confidence interval includes Zero, so it's not statistically significant. It includes the lower boundary for practical significance, so it's also not practically significant.

#### Sign Tests
To do a sign test we can use the binom.test function in R. I first create columns in the dataframe to make it easier to calculate the difference. I then count the occurences of negative and positive values. 

##### Gross Conversion
```{r eval=FALSE, echo=FALSE}
experiment_na$gc <- experiment_na$Enrollments/experiment_na$Clicks
control_na$gc <- control_na$Enrollments/control_na$Clicks

gc_values <- experiment_na$gc - control_na$gc
length(gc_values)
length(gc_values[gc_values > 0])
```

There were 23 days recorded and 19 days in which the probability of enrollment for the experiment group is lower than for the control group.

```{r}
binom.test(4,23)
```

The binomical test gives us a p-value = 0.002599, so the result is statistically significant.

##### Net Conversion
```{r eval=FALSE, echo=FALSE}
experiment_na$nc <- experiment_na$Payments/experiment_na$Clicks
control_na$nc <- control_na$Payments/control_na$Clicks

nc_values <- experiment_na$nc - control_na$nc
length(nc_values)
length(nc_values[nc_values > 0])
```
Like above, 23 days were recorded, but here we had only 10 days with an improvement.


```{r}
binom.test(10,23)
```

The p-value = 0.6776, so the difference is not statistically significant.

#### Summary
The main result of the experiment is that the gross conversion did decrease significantly. This supports the hypothesis. However, the net conversion had a confidence interval above and below zero and ist not statically significant for that reason.

For this experiment the Bonferroni correction was not used as I all metrics have to be statistically significant to recommend the changes done in this experiment.

### Recommendation
There are two main points we have learned from the experiment:

* Less users enroll in the free trial: the Gross conversion is statistically and practically lower, so we can assume that the warning regarding the required working hours has an effect.
* The results of the Net conversion are neither stastically nor practically significant. Furthermore, the confidence interval includes negative values, which indicates that the change could even have a negative influence (i.e. a lower amount of paid subscriptions).

Therefore, it is *not* recommended to implement the change.

## Follow-Up Experiment
In this experiment we only focussed on the time a user is able to spend on the course and considered this as the main reason for users to cancel their subscription before the end of the trial period.
However, I think there are many more reasons why users cancel. One, which I consider quite likely for Nanodegree programs is that user are just overchallenged by the long list of course material. They might also have problems to start with a topic because they are missing basics.

My idea would be to contact users after half of the trial period (i.e. 7 days) and invite users to get some expert advice in case there are any problems with the content. This does not necessarily require an individual personal guidance, but an in-depth 

I would use the same metrics and the same diversion like in this experiment. My null hypothesis would be: Sending users emails to invite them to get some expert advice will not increase the number of enrollments.

## Ressources
* Udacity course material